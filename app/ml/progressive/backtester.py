"""
Progressive Backtester for Date-Range Training & Validation
===========================================================
Advanced backtesting system with iterative training and accuracy improvement

Features:
- Date-range based training and testing
- Iterative model improvement with expanding windows
- Automatic accuracy tracking and comparison with real data
- Auto-stop when target accuracy reached
- Save all models from each iteration
- Comprehensive results tracking and visualization data
"""

import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Callable
from datetime import datetime, timedelta
import time
import uuid

from .data_loader import ProgressiveDataLoader
from .trainer import ProgressiveTrainer
from .predictor import ProgressivePredictor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProgressiveBacktester:
    """
    Progressive Backtesting System
    
    Trains models iteratively with expanding time windows and evaluates
    against real data to achieve target accuracy
    """
    
    def __init__(self, 
                 data_loader: ProgressiveDataLoader,
                 trainer: ProgressiveTrainer,
                 predictor: ProgressivePredictor,
                 config: Optional[Dict] = None,
                 progress_callback: Optional[Callable[[Dict], None]] = None,
                 cancel_checker: Optional[Callable[[], bool]] = None):
        """
        Initialize Progressive Backtester
        
        Args:
            data_loader: ProgressiveDataLoader instance
            trainer: ProgressiveTrainer instance
            predictor: ProgressivePredictor instance
            config: Optional configuration dictionary
        """
        self.data_loader = data_loader
        self.trainer = trainer
        self.predictor = predictor
        
        # Calculate absolute path for results directory
        # __file__ = d:/Projects/NewsFetcher/app/ml/progressive/backtester.py
        # .parent = progressive, .parent.parent = ml, .parent.parent.parent = app
        # .parent.parent.parent.parent = NewsFetcher root
        base_dir = Path(__file__).parent.parent.parent.parent  # Goes to NewsFetcher root
        default_results_dir = str(base_dir / "app" / "ml" / "models" / "backtest_results")
        
        # Default config
        self.config = {
            'save_all_models': True,
            'save_results': True,
            'results_dir': default_results_dir,
            'verbose': True
        }
        
        if config:
            self.config.update(config)
        
        # Create results directory
        Path(self.config['results_dir']).mkdir(parents=True, exist_ok=True)
        
        # State tracking
        self.current_job_id = None
        self.iteration_results = []
        self.best_iteration = None
        self.is_running = False
        self.progress_callback = progress_callback
        self.cancel_checker = cancel_checker
        self.job_model_dir: Optional[Path] = None
        
        logger.info("ðŸ”¬ Progressive Backtester initialized")
        logger.info(f"   ðŸ“‚ Results directory: {self.config['results_dir']}")
    
    def run_backtest(self,
                     symbol: str,
                     train_start_date: str,
                     train_end_date: str,
                     test_period_days: int = 14,
                     max_iterations: int = 10,
                     target_accuracy: float = 0.85,
                     auto_stop: bool = True,
                     model_types: List[str] = ['lstm']) -> Dict:
        """
        Run complete backtesting process
        
        Args:
            symbol: Stock symbol to train on
            train_start_date: Initial training start date (YYYY-MM-DD)
            train_end_date: Initial training end date (YYYY-MM-DD)
            test_period_days: Number of days to test forward
            max_iterations: Maximum iterations to run
            target_accuracy: Target accuracy to achieve (0-1)
            auto_stop: Stop when target accuracy reached
            model_types: List of model types to train
            
        Returns:
            Dictionary with complete backtest results
        """
        
    # Generate unique job ID
        self.current_job_id = f"backtest_{symbol}_{uuid.uuid4().hex[:8]}"
        self.is_running = True
        self.iteration_results = []
        
        logger.info("=" * 70)
        logger.info(f"ðŸ”¬ STARTING BACKTEST: {self.current_job_id}")
        logger.info("=" * 70)
        logger.info(f"ðŸ“Š Symbol: {symbol}")
        logger.info(f"ðŸ“… Training: {train_start_date} â†’ {train_end_date}")
        logger.info(f"ðŸ§ª Test period: {test_period_days} days")
        logger.info(f"ðŸ”„ Max iterations: {max_iterations}")
        logger.info(f"ðŸŽ¯ Target accuracy: {target_accuracy * 100}%")
        logger.info(f"â¹ Auto-stop: {'Enabled' if auto_stop else 'Disabled'}")
        logger.info("=" * 70)
        
        start_time = time.time()
        current_train_end = train_end_date

        # Create a dedicated model directory for this backtest job to avoid overwriting production checkpoints
        try:
            base_dir = Path(__file__).parent.parent.parent.parent
            self.job_model_dir = base_dir / "app" / "ml" / "models" / "backtests" / self.current_job_id
            self.job_model_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"   ðŸ“‚ Job model dir: {self.job_model_dir}")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not create job-specific model dir: {e}")
            self.job_model_dir = Path(self.config.get('results_dir', '.')) / self.current_job_id
            try:
                self.job_model_dir.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
        
        # Load full data for the symbol (no date restrictions yet)
        full_loader = ProgressiveDataLoader(
            stock_data_dir=self.data_loader.stock_data_dir,
            sequence_length=self.data_loader.sequence_length,
            horizons=self.data_loader.horizons,
            use_fundamentals=self.data_loader.use_fundamentals,
            use_technical_indicators=self.data_loader.use_technical_indicators,
            indicator_params=getattr(self.data_loader, 'indicator_params', None)
        )
        full_df = full_loader.load_stock_data(symbol)
        
        if full_df is None:
            logger.error(f"âŒ Failed to load data for {symbol}")
            self.is_running = False
            return self._create_error_result("Failed to load stock data")
        
        # Validate dates
        if not self.data_loader.validate_date_range(train_start_date, train_end_date):
            logger.error("âŒ Invalid date range")
            self.is_running = False
            return self._create_error_result("Invalid date range")
        
        # Run iterations
        for iteration in range(1, max_iterations + 1):
            # Check cancellation
            if self.cancel_checker and self.cancel_checker():
                self.is_running = False
                # Notify progress callback
                if self.progress_callback:
                    elapsed = time.time() - start_time
                    self.progress_callback({
                        'status': 'cancelled',
                        'iteration': iteration,
                        'total_iterations': max_iterations,
                        'progress': int(((iteration - 1) / max_iterations) * 100),
                        'current_step': 'Backtest cancelled by user',
                        'elapsed_seconds': int(elapsed)
                    })
                summary = {
                    'status': 'cancelled',
                    'job_id': self.current_job_id,
                    'symbol': symbol,
                    'total_iterations': len(self.iteration_results),
                    'all_iterations': self.iteration_results,
                    'timestamp': datetime.now().isoformat()
                }
                return summary
            logger.info(f"\n{'='*70}")
            logger.info(f"ðŸ”„ ITERATION {iteration}/{max_iterations}")
            logger.info(f"{'='*70}")
            # Progress update: starting iteration
            if self.progress_callback:
                elapsed = time.time() - start_time
                self.progress_callback({
                    'status': 'running',
                    'iteration': iteration,
                    'total_iterations': max_iterations,
                    'progress': int((iteration - 1) / max_iterations * 100),
                    'current_step': f'Starting iteration {iteration}/{max_iterations}...',
                    'elapsed_seconds': int(elapsed)
                })
            
            try:
                # Train iteration
                iteration_result = self.train_iteration(
                    symbol=symbol,
                    train_start_date=train_start_date,
                    train_end_date=current_train_end,
                    iteration_num=iteration,
                    model_types=model_types,
                    full_df=full_df
                )
                
                if not iteration_result['success']:
                    logger.error(f"âŒ Iteration {iteration} failed: {iteration_result.get('error', 'Unknown error')}")
                    break
                
                # Evaluate iteration
                test_start_date = self._add_days(current_train_end, 1)
                test_end_date = self._add_days(test_start_date, test_period_days)
                # Progress update: evaluating
                if self.progress_callback:
                    elapsed = time.time() - start_time
                    self.progress_callback({
                        'status': 'running',
                        'iteration': iteration,
                        'total_iterations': max_iterations,
                        'progress': int(((iteration - 0.5) / max_iterations) * 100),
                        'current_step': f'Evaluating iteration {iteration} on test window...',
                        'elapsed_seconds': int(elapsed)
                    })
                
                evaluation = self.evaluate_iteration(
                    symbol=symbol,
                    test_start_date=test_start_date,
                    test_end_date=test_end_date,
                    iteration_num=iteration,
                    full_df=full_df
                )
                
                # Combine results
                combined_result = {
                    **iteration_result,
                    **evaluation,
                    'iteration': iteration,
                    'train_until': current_train_end,
                    'test_period': f"{test_start_date} to {test_end_date}"
                }
                
                self.iteration_results.append(combined_result)
                
                # Save iteration results
                if self.config['save_results']:
                    self.save_iteration_results(symbol, combined_result)
                
                # Log results
                logger.info(f"âœ… Iteration {iteration} completed")
                logger.info(f"   ðŸ“Š Accuracy: {evaluation['accuracy']:.2%}")
                loss_val = iteration_result.get('val_loss')
                loss_str = f"{loss_val:.4f}" if isinstance(loss_val, (int, float)) and loss_val is not None else "N/A"
                logger.info(f"   ðŸ“‰ Loss: {loss_str}")
                logger.info(f"   â± Time: {iteration_result['training_time']:.1f}s")
                
                # Stop early if there's no evaluable test data (e.g., window moved past available market dates)
                try:
                    if int(evaluation.get('test_samples', 0) or 0) == 0:
                        logger.info("ðŸ›‘ No test data available for this window; stopping iterations early")
                        break
                except Exception:
                    pass

                # Check if we should continue
                if not self.should_continue(
                    current_accuracy=evaluation['accuracy'],
                    target_accuracy=target_accuracy,
                    iteration=iteration,
                    max_iterations=max_iterations,
                    auto_stop=auto_stop
                ):
                    logger.info(f"ðŸŽ¯ Target accuracy reached! Stopping at iteration {iteration}")
                    # Progress update: target reached
                    if self.progress_callback:
                        elapsed = time.time() - start_time
                        self.progress_callback({
                            'status': 'running',
                            'iteration': iteration,
                            'total_iterations': max_iterations,
                            'progress': 95,
                            'current_step': 'Target accuracy reached, finalizing...',
                            'elapsed_seconds': int(elapsed)
                        })
                    break
                
                # Expand training window for next iteration
                current_train_end = test_end_date
                
            except Exception as e:
                logger.error(f"âŒ Error in iteration {iteration}: {e}")
                break
        
        # Calculate summary
        total_time = time.time() - start_time
        summary = self.get_backtest_summary(symbol, total_time)
        
        self.is_running = False
        
        logger.info("\n" + "=" * 70)
        if summary['status'] == 'completed':
            logger.info("ðŸŽ‰ BACKTEST COMPLETED")
            logger.info("=" * 70)
            logger.info(f"ðŸ“Š Total iterations: {len(self.iteration_results)}")
            logger.info(f"ðŸ† Best accuracy: {summary['best_accuracy']:.2%} (Iteration {summary['best_iteration']})")
            logger.info(f"â± Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
        else:
            logger.error("âŒ BACKTEST FAILED")
            logger.error("=" * 70)
            logger.error(f"âŒ Error: {summary.get('error', 'Unknown error')}")
            logger.error(f"â± Time: {total_time:.1f}s")
        logger.info("=" * 70)
        
        # Final progress update
        if self.progress_callback:
            elapsed = time.time() - start_time
            self.progress_callback({
                'status': 'completed',
                'progress': 100,
                'current_step': 'âœ… Backtest completed successfully!',
                'elapsed_seconds': int(elapsed),
                'result': summary
            })

        return summary
    
    def train_iteration(self,
                       symbol: str,
                       train_start_date: str,
                       train_end_date: str,
                       iteration_num: int,
                       model_types: List[str],
                       full_df: pd.DataFrame) -> Dict:
        """
        Train models for one iteration
        
        Args:
            symbol: Stock symbol
            train_start_date: Training start date
            train_end_date: Training end date
            iteration_num: Current iteration number
            model_types: List of model types to train
            full_df: Full dataframe with all data
            
        Returns:
            Dictionary with training results
        """
        logger.info(f"ðŸ“š Training on: {train_start_date} â†’ {train_end_date}")
        
        try:
            # Create data loader with date restrictions
            iteration_loader = ProgressiveDataLoader(
                stock_data_dir=self.data_loader.stock_data_dir,
                sequence_length=self.data_loader.sequence_length,
                horizons=self.data_loader.horizons,
                train_start_date=train_start_date,
                train_end_date=train_end_date,
                use_fundamentals=self.data_loader.use_fundamentals,
                use_technical_indicators=self.data_loader.use_technical_indicators,
                indicator_params=getattr(self.data_loader, 'indicator_params', None)
            )
            
            # Prepare features
            features_df = iteration_loader.prepare_features(symbol)
            
            if features_df is None:
                return {
                    'success': False,
                    'error': 'Failed to load features data'
                }
            
            if len(features_df) < 50:  # Lower threshold for backtesting
                return {
                    'success': False,
                    'error': f'Insufficient data for training: {len(features_df)} samples (need â‰¥50)'
                }
            
            # Create sequences
            sequences = iteration_loader.create_sequences(features_df, mode='progressive')
            
            # Split data
            split_data = iteration_loader.split_data(sequences)
            
            # REAL TRAINING - Use the existing trainer with custom data
            training_start = time.time()
            
            logger.info(f"ðŸš€ Starting REAL training for {len(model_types)} model(s)...")
            logger.info(f"ðŸ“Š Training data: {len(features_df)} samples")
            
            # Create a temporary trainer instance with this specific data
            # Use per-iteration subfolder so each iteration's checkpoints are preserved
            iter_save_dir = self.job_model_dir / f"iter_{iteration_num:02d}" if self.job_model_dir is not None else self.trainer.save_dir
            try:
                Path(iter_save_dir).mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            temp_trainer = ProgressiveTrainer(
                data_loader=iteration_loader,  # Use our custom data loader
                model_config=self.trainer.model_config,
                training_config=self.trainer.training_config,
                # Save checkpoints to iteration-specific directory to prevent overwriting
                save_dir=str(iter_save_dir)
            )
            
            # Train models using the real training system
            training_results = temp_trainer.train_progressive_models(
                symbol=symbol, 
                model_types=model_types
            )
            
            training_time = time.time() - training_start
            
            # Extract results from the real training
            # Trainer normalizes some UI aliases (e.g., 'cnn_lstm' -> 'cnn'). Try both keys.
            model_results = {}
            if model_types:
                primary_key = model_types[0]
                candidates = [primary_key]
                try:
                    normalizer = getattr(self.trainer, '_normalize_model_type', None)
                    if callable(normalizer):
                        normalized = normalizer(primary_key)
                        if normalized and normalized not in candidates:
                            candidates.append(normalized)
                except Exception:
                    pass
                for k in candidates:
                    if isinstance(training_results, dict) and k in training_results:
                        model_results = training_results.get(k, {})
                        break
            best_horizon = '1d'  # Use 1-day predictions for backtesting
            horizon_results = model_results.get(best_horizon, {})
            
            # Extract real validation loss from training results
            real_val_loss = None  # live-only: do not use fallback constants
            if model_results and best_horizon in model_results:
                horizon_data = model_results[best_horizon]
                if 'final_loss' in horizon_data:
                    real_val_loss = horizon_data['final_loss']
                elif 'history' in horizon_data and 'val_loss' in horizon_data['history']:
                    val_losses = horizon_data['history']['val_loss']
                    if val_losses:
                        real_val_loss = val_losses[-1]  # Last validation loss
            
            training_result = {
                'success': True,
                'train_samples': len(features_df),
                'training_time': training_time,
                'val_loss': real_val_loss,
                'models_trained': model_types,
                'model_path': horizon_results.get('model_path', ''),
                'training_results': training_results  # Keep full results for analysis
            }
            
            # Save model with iteration number
            if self.config['save_all_models']:
                model_path = Path(self.config['results_dir']) / f"model_{symbol}_iter{iteration_num}.h5"
                # Model saving will be implemented when we integrate with actual training
                logger.info(f"ðŸ’¾ Model saved: {model_path.name}")
            
            return training_result
            
        except Exception as e:
            logger.error(f"âŒ Training error: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def evaluate_iteration(self,
                          symbol: str,
                          test_start_date: str,
                          test_end_date: str,
                          iteration_num: int,
                          full_df: pd.DataFrame) -> Dict:
        """
        Evaluate model on test period against real data
        
        Args:
            symbol: Stock symbol
            test_start_date: Test period start date
            test_end_date: Test period end date
            iteration_num: Current iteration number
            full_df: Full dataframe with all data
            
        Returns:
            Dictionary with evaluation metrics
        """
        logger.info(f"ðŸ§ª Testing on: {test_start_date} â†’ {test_end_date}")
        
        try:
            # Get real data for test period
            test_df = full_df[(full_df.index >= test_start_date) & (full_df.index <= test_end_date)]
            
            if len(test_df) == 0:
                return {
                    'accuracy': 0.0,
                    'error': 'No test data available'
                }
            
            # REAL EVALUATION - Use the predictor to make actual predictions per test day
            logger.info(f"ðŸ”® Making real predictions for {len(test_df)} test samples (sliding window)...")

            pred_prices = []
            act_prices = []
            pred_dirs = []
            act_dirs = []
            predictions_made = 0

            # Temporarily switch predictor to use this iteration's model_dir and clear any cached models
            original_model_dir = getattr(self.predictor, 'model_dir', None)
            try:
                if self.job_model_dir is not None:
                    # Prefer the current iteration's checkpoint directory
                    iter_dir = Path(self.job_model_dir) / f"iter_{iteration_num:02d}"
                    effective_dir = iter_dir if iter_dir.exists() else Path(self.job_model_dir)
                    self.predictor.model_dir = effective_dir
                    # Clear loaded models cache to force reload from the job directory
                    self.predictor.loaded_models = {}
            except Exception:
                pass

            # Iterate per test day; for each, restrict data up to that day and predict next-day (1d horizon)
            for test_date in test_df.index:
                # Check cancellation during evaluation loop
                if self.cancel_checker and self.cancel_checker():
                    logger.info("ðŸ›‘ Cancellation detected during evaluation loop")
                    break
                try:
                    # Prepare a data loader limited to data up to test_date
                    perday_loader = ProgressiveDataLoader(
                        stock_data_dir=self.data_loader.stock_data_dir,
                        sequence_length=self.data_loader.sequence_length,
                        horizons=self.data_loader.horizons,
                        train_end_date=str(test_date.date()),
                        use_fundamentals=self.data_loader.use_fundamentals,
                        use_technical_indicators=self.data_loader.use_technical_indicators,
                        indicator_params=getattr(self.data_loader, 'indicator_params', None)
                    )
                    # Temporarily swap predictor's data loader
                    original_loader = self.predictor.data_loader
                    self.predictor.data_loader = perday_loader

                    prediction_result = self.predictor.predict_ensemble(
                        symbol=symbol,
                        mode="progressive"
                    )

                    # Restore original loader
                    self.predictor.data_loader = original_loader

                    if prediction_result and isinstance(prediction_result, dict):
                        pred_1d = prediction_result.get('predictions', {}).get('1d')
                        if pred_1d:
                            # Use top-level current_price (per our predictor structure)
                            current_price = float(prediction_result.get('current_price', 0.0))
                            change_pct = float(pred_1d.get('price_change_pct', 0.0))
                            predicted_price = float(current_price * (1.0 + change_pct))

                            # Actual next-day price
                            # Find next index after test_date in full_df
                            if test_date in full_df.index:
                                idx = full_df.index.get_loc(test_date)
                                if isinstance(idx, (int, np.integer)) and idx + 1 < len(full_df.index):
                                    next_idx = idx + 1
                                    actual_today = float(full_df.iloc[idx]['Close']) if 'Close' in full_df.columns else None
                                    actual_next = float(full_df.iloc[next_idx]['Close']) if 'Close' in full_df.columns else None
                                    if actual_today is not None and actual_next is not None:
                                        pred_prices.append(predicted_price)
                                        act_prices.append(actual_next)
                                        pred_dirs.append(1 if change_pct > 0 else 0)
                                        act_dirs.append(1 if (actual_next - actual_today) > 0 else 0)
                                        predictions_made += 1
                except Exception as pred_error:
                    logger.warning(f"âš ï¸ Prediction failed for {test_date}: {pred_error}")
                    # Ensure predictor loader is restored
                    try:
                        self.predictor.data_loader = original_loader
                    except Exception:
                        pass
                    continue

            # Restore predictor model_dir at the end of evaluation
            try:
                if original_model_dir is not None:
                    self.predictor.model_dir = original_model_dir
                    # Keep caches cleared to avoid stale handles
                    self.predictor.loaded_models = {}
            except Exception:
                pass

            if predictions_made == 0:
                # Live-only: no fabricated improvements; report no predictions
                return {
                    'accuracy': 0.0,
                    'test_samples': 0,
                    'mae': None,
                    'rmse': None,
                    'mape': None,
                    'direction_accuracy': 0.0,
                    'predictions_made': 0,
                    'total_test_days': len(test_df),
                    'note': 'No model predictions could be generated for the test period'
                }

            # Compute metrics
            pred_prices_np = np.array(pred_prices)
            act_prices_np = np.array(act_prices)
            pred_dirs_np = np.array(pred_dirs)
            act_dirs_np = np.array(act_dirs)

            direction_accuracy = float(np.mean(pred_dirs_np == act_dirs_np)) if len(pred_dirs_np) > 0 else 0.0
            mae = float(np.mean(np.abs(pred_prices_np - act_prices_np)))
            rmse = float(np.sqrt(np.mean((pred_prices_np - act_prices_np) ** 2)))
            mape = float(np.mean(np.abs((pred_prices_np - act_prices_np) / np.maximum(act_prices_np, 1e-8))) * 100.0)

            logger.info(f"âœ… Evaluation complete: {predictions_made} predictions made")
            logger.info(f"ðŸ“Š Direction accuracy: {direction_accuracy:.2%}")
            logger.info(f"ðŸ“Š MAPE: {mape:.2f}%")

            return {
                'accuracy': direction_accuracy,
                'test_samples': int(predictions_made),
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'direction_accuracy': direction_accuracy,
                'predictions_made': int(predictions_made),
                'total_test_days': len(test_df)
            }
            
        except Exception as e:
            logger.error(f"âŒ Evaluation error: {e}")
            return {
                'accuracy': 0.0,
                'error': str(e)
            }
    
    # calculate_accuracy function removed - now using real predictions in evaluate_iteration
    
    def should_continue(self,
                       current_accuracy: float,
                       target_accuracy: float,
                       iteration: int,
                       max_iterations: int,
                       auto_stop: bool) -> bool:
        """
        Determine if backtesting should continue
        
        Args:
            current_accuracy: Current iteration accuracy
            target_accuracy: Target accuracy to achieve
            iteration: Current iteration number
            max_iterations: Maximum iterations allowed
            auto_stop: Whether to auto-stop when target reached
            
        Returns:
            True if should continue, False otherwise
        """
        # Reached max iterations
        if iteration >= max_iterations:
            logger.info(f"ðŸ›‘ Reached max iterations ({max_iterations})")
            return False
        
        # Target accuracy reached and auto-stop enabled
        if auto_stop and current_accuracy >= target_accuracy:
            logger.info(f"ðŸŽ¯ Target accuracy ({target_accuracy:.2%}) reached!")
            return False
        
        return True
    
    def save_iteration_results(self, symbol: str, iteration_data: Dict):
        """
        Save results from a single iteration
        
        Args:
            symbol: Stock symbol
            iteration_data: Dictionary with iteration results
        """
        try:
            results_file = Path(self.config['results_dir']) / f"results_{symbol}_{self.current_job_id}.json"
            
            # Load existing results if any
            if results_file.exists():
                with open(results_file, 'r') as f:
                    all_results = json.load(f)
            else:
                all_results = {
                    'job_id': self.current_job_id,
                    'symbol': symbol,
                    'start_time': datetime.now().isoformat(),
                    'iterations': []
                }
            
            # Add new iteration
            all_results['iterations'].append(iteration_data)
            
            # Save updated results
            with open(results_file, 'w') as f:
                json.dump(all_results, f, indent=2)
            
            logger.info(f"ðŸ’¾ Results saved: {results_file.name}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving results: {e}")
    
    def get_backtest_summary(self, symbol: str, total_time: float) -> Dict:
        """
        Generate summary of backtest results
        
        Args:
            symbol: Stock symbol
            total_time: Total time taken
            
        Returns:
            Dictionary with summary statistics
        """
        if not self.iteration_results:
            return {
                'status': 'failed',
                'error': 'No iterations completed',
                'job_id': self.current_job_id,
                'symbol': symbol,
                'total_iterations': 0,
                'total_time': total_time,
                'timestamp': datetime.now().isoformat()
            }
        
        # Find best iteration
        best_iter = max(self.iteration_results, key=lambda x: x.get('accuracy', 0))
        best_iteration_num = best_iter['iteration']
        
        # Calculate statistics
        accuracies = [r['accuracy'] for r in self.iteration_results if 'accuracy' in r]
        
        summary = {
            'status': 'completed',
            'job_id': self.current_job_id,
            'symbol': symbol,
            'total_iterations': len(self.iteration_results),
            'total_time': total_time,
            'best_iteration': best_iteration_num,
            'best_accuracy': best_iter['accuracy'],
            'best_loss': best_iter.get('val_loss', 0),
            'accuracy_improvement': accuracies[-1] - accuracies[0] if len(accuracies) > 1 else 0,
            'all_iterations': self.iteration_results,
            'timestamp': datetime.now().isoformat()
        }
        
        return summary
    
    def get_status(self) -> Dict:
        """
        Get current backtesting status
        
        Returns:
            Dictionary with current status
        """
        if not self.current_job_id:
            return {
                'is_running': False,
                'message': 'No backtest running'
            }
        
        return {
            'is_running': self.is_running,
            'job_id': self.current_job_id,
            'iterations_completed': len(self.iteration_results),
            'current_best_accuracy': max([r['accuracy'] for r in self.iteration_results]) if self.iteration_results else 0
        }
    
    def _add_days(self, date_str: str, days: int) -> str:
        """Helper to add days to a date string"""
        date = pd.to_datetime(date_str)
        new_date = date + timedelta(days=days)
        return new_date.strftime('%Y-%m-%d')
    
    def _create_error_result(self, error_message: str) -> Dict:
        """Helper to create error result dictionary"""
        return {
            'status': 'failed',
            'error': error_message,
            'job_id': self.current_job_id,
            'timestamp': datetime.now().isoformat()
        }
